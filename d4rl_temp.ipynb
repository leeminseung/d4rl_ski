{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\n",
    "import d4rl # Import required to register environments\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('halfcheetah-medium-expert-v2')\n",
    "\n",
    "# d4rl abides by the OpenAI gym interface\n",
    "env.reset()\n",
    "env.step(env.action_space.sample())\n",
    "\n",
    "# Each task is associated with a dataset\n",
    "# dataset contains observations, actions, rewards, terminals, and infos\n",
    "dataset = env.get_dataset()\n",
    "print(dataset['observations']) # An N x dim_observation Numpy array of observations\n",
    "\n",
    "# Alternatively, use d4rl.qlearning_dataset which\n",
    "# also adds next_observations.\n",
    "dataset = d4rl.qlearning_dataset(env)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "/home/lms/anaconda3/envs/mujoco/lib/python3.7/site-packages/glfw/__init__.py:834: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "/home/lms/anaconda3/envs/mujoco/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/gym_mujoco_v2/halfcheetah_medium_expert-v2.hdf5 to /home/lms/.d4rl/datasets/halfcheetah_medium_expert-v2.hdf5\n",
      "[[ 1.9831914e-02 -8.9501314e-02 -3.1969063e-03 ...  1.1365079e-01\n",
      "   6.8424918e-02 -1.3811582e-01]\n",
      " [-3.8486063e-03 -5.2394319e-02  8.3050327e-03 ...  4.5068407e+00\n",
      "  -9.2885571e+00  4.7328596e+00]\n",
      " [-5.5298433e-02 -7.7850236e-05 -2.3952831e-01 ... -7.0811687e+00\n",
      "  -1.4037068e+00  7.5524049e+00]\n",
      " ...\n",
      " [-3.8276739e-02 -5.9685200e-03 -5.3859454e-01 ...  9.6563587e+00\n",
      "  -9.2510633e+00 -2.3956337e+00]\n",
      " [-3.5350587e-02 -1.3052115e-01 -1.6677204e-01 ... -3.3741906e+00\n",
      "  -4.8845510e+00 -5.1081996e+00]\n",
      " [-9.0780985e-03 -1.5547317e-01  6.0090959e-01 ... -2.2751564e+01\n",
      "  -3.7737691e+00 -3.9162564e+00]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "dataset.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['observations', 'actions', 'next_observations', 'rewards', 'terminals'])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "dataset['observations'].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1998000, 17)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import torch.nn as nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "class BigEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BigEncoder, self).__init__()\n",
    "         \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "class StochasticTransitionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, minimum_std = 0.001):\n",
    "        super(StochasticTransitionModel, self).__init__()\n",
    "\n",
    "        self.make_prob_parameters = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 2 * output_size)\n",
    "        )\n",
    "\n",
    "        self.minimum_std = minimum_std\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        flatted_mu_std = self.make_prob_parameters(x)\n",
    "        reshaped_mu_std = flatted_mu_std.reshape(-1, self.output_size, 2)\n",
    "        mu = reshaped_mu_std[:, :, 0]\n",
    "        std = reshaped_mu_std[:, :, 1]\n",
    "\n",
    "        std += self.minimum_std\n",
    "        epsilon = torch.randn((x.shape[0], self.output_size))\n",
    "        next_state_prediction = epsilon * std + mu\n",
    "        return next_state_prediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class EntireEnsembleModel(nn.Module):\n",
    "    def __init__(self, input_size, encoder_hidden_size, encoder_output_size, transition_model_hidden_size, transition_model_output_size, ensemble_size, learning_rate):\n",
    "        super(EntireEnsembleModel, self).__init__()\n",
    "        \n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.big_encoder = BigEncoder(input_size, encoder_hidden_size, encoder_output_size)\n",
    "        self.stochastic_models = list()\n",
    "        for _ in range(ensemble_size):\n",
    "            self.stochastic_models.append(StochasticTransitionModel(encoder_hidden_size, transition_model_hidden_size, transition_model_output_size))\n",
    "        \n",
    "        self.all_parameters = list(self.big_encoder.parameters())\n",
    "        for idx in range(ensemble_size):\n",
    "            self.all_parameters += list(self.stochastic_models[idx].parameters())\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.all_parameters, lr=learning_rate)\n",
    "        self.ensemble_size = ensemble_size\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat((state, action), dim=1)\n",
    "        latent = self.big_encoder(state_action)\n",
    "        selected_model_idx = random.choice(range(len(self.stochastic_models)))\n",
    "        selected_model =self.stochastic_models[selected_model_idx]\n",
    "        next_state_prediction = selected_model(latent)\n",
    "        return next_state_prediction\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.big_encoder.state_dict(), os.path.join(\"model_pt\", path + \"_big_encoder.pt\"))\n",
    "        for idx in range(self.ensemble_size):\n",
    "            torch.save(self.stochastic_models[idx].state_dict(), os.path.join(\"model_pt\", path + \"_ensemble_{}.pt\".format(idx)))\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.big_encoder.load_state_dict(torch.load(os.path.join(\"model_pt\", path + \"_big_encoder.pt\")))\n",
    "        for idx in range(self.ensemble_size):        \n",
    "            self.stochastic_models[idx].load_state_dict(torch.load(os.path.join(\"model_pt\", path + \"_ensemble_{}.pt\".format(idx))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "path = \"0804\"\n",
    "torch.save(model.big_encoder.state_dict(), os.path.join(\"model_pt\", path + \"_big_encoder.pt\"))\n",
    "for idx in range(model.ensemble_size):\n",
    "    torch.save(model.stochastic_models[idx].state_dict(),os.path.join(\"model_pt\", path + \"_ensemble_{}.pt\".format(idx)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class D4rlDataset(nn.Module):\n",
    "    def __init__(self, d4rl_dataset):\n",
    "        super(D4rlDataset, self).__init__()\n",
    "        self.d4rl_dataset = d4rl_dataset\n",
    "        self.state_array = d4rl_dataset['observations']\n",
    "        self.next_state_array = d4rl_dataset['next_observations']\n",
    "        self.action_array = d4rl_dataset['actions']\n",
    "        self.reward_array = d4rl_dataset['rewards']\n",
    "        self.terminal_array = d4rl_dataset['terminals']        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'state': self.state_array[idx], 'action': self.action_array[idx], 'next_state': self.next_state_array[idx]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.state_array.shape[0]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "model2.load_model(path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "whole_ind = np.arange(dataset['observations'].shape[0])\n",
    "train_ind, val_ind = train_test_split(whole_ind, test_size=0.1, random_state = 42)\n",
    "train_ind, test_ind = train_test_split(train_ind, test_size=0.1, random_state = 42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "train_dataset = {}\n",
    "val_dataset = {}\n",
    "test_dataset = {}\n",
    "\n",
    "for key in dataset.keys():\n",
    "    train_dataset[key] = dataset[key][train_ind]\n",
    "    val_dataset[key] = dataset[key][val_ind]\n",
    "    test_dataset[key] = dataset[key][test_ind]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "d4rl_train_dataset = D4rlDataset(dataset)\n",
    "d4rl_train_dataloader = DataLoader(d4rl_train_dataset, batch_size=256, shuffle=True, drop_last=False)\n",
    "\n",
    "d4rl_val_dataset = D4rlDataset(dataset)\n",
    "d4rl_val_dataloader = DataLoader(d4rl_val_dataset, batch_size=256, shuffle=True, drop_last=False)\n",
    "\n",
    "d4rl_test_dataset = D4rlDataset(dataset)\n",
    "d4rl_test_dataloader = DataLoader(d4rl_test_dataset, batch_size=256, shuffle=True, drop_last=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "model = EntireEnsembleModel(input_size=17+6, encoder_hidden_size=64, encoder_output_size=64, transition_model_hidden_size=64, transition_model_output_size=17, ensemble_size=5, learning_rate=0.0005)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "model1 = model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0\n",
    "    for row in d4rl_train_dataloader:\n",
    "        state = row['state']\n",
    "        action = row['action']\n",
    "        next_state = row['next_state']\n",
    "\n",
    "        next_state_prediction = model(state, action)\n",
    "        loss = loss_fn(next_state, next_state_prediction)\n",
    "\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        model.optimizer.zero_grad()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "    print('Train Mean Loss at {} epoch: {}'.format(loss_sum/len(d4rl_train_dataloader), epoch))\n",
    "\n",
    "    loss_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for row in d4rl_val_dataloader:\n",
    "            state = row['state']\n",
    "            action = row['action']\n",
    "            next_state = row['next_state']\n",
    "\n",
    "            next_state_prediction = model(state, action)\n",
    "            loss = loss_fn(next_state, next_state_prediction)\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "    print('Validation Mean Loss at {} epoch: {}'.format(loss_sum/len(d4rl_train_dataloader), epoch))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Mean Loss at 3.160057742247438 epoch: 0\n",
      "Validation Mean Loss at 1.3023283789587052 epoch: 0\n",
      "Train Mean Loss at 1.1191813480999133 epoch: 1\n",
      "Validation Mean Loss at 0.9346889226655025 epoch: 1\n",
      "Train Mean Loss at 0.9138090314581027 epoch: 2\n",
      "Validation Mean Loss at 1.0948217041365693 epoch: 2\n",
      "Train Mean Loss at 0.7841161046442048 epoch: 3\n",
      "Validation Mean Loss at 0.7120429783544046 epoch: 3\n",
      "Train Mean Loss at 0.6987513205510236 epoch: 4\n",
      "Validation Mean Loss at 0.640969680128458 epoch: 4\n",
      "Train Mean Loss at 0.6271592181054236 epoch: 5\n",
      "Validation Mean Loss at 0.5726932004046394 epoch: 5\n",
      "Train Mean Loss at 0.5807639912219478 epoch: 6\n",
      "Validation Mean Loss at 0.5678961675988795 epoch: 6\n",
      "Train Mean Loss at 0.5305012987868134 epoch: 7\n",
      "Validation Mean Loss at 0.4834953626786036 epoch: 7\n",
      "Train Mean Loss at 0.49389000348293344 epoch: 8\n",
      "Validation Mean Loss at 0.4554047950409224 epoch: 8\n",
      "Train Mean Loss at 0.46626118687312623 epoch: 9\n",
      "Validation Mean Loss at 0.404511248380003 epoch: 9\n",
      "Train Mean Loss at 0.4290991333568043 epoch: 10\n",
      "Validation Mean Loss at 0.4273654321384002 epoch: 10\n",
      "Train Mean Loss at 0.40764137265737205 epoch: 11\n",
      "Validation Mean Loss at 0.3959088801249413 epoch: 11\n",
      "Train Mean Loss at 0.3865573474281801 epoch: 12\n",
      "Validation Mean Loss at 0.3876435365052959 epoch: 12\n",
      "Train Mean Loss at 0.36828823585414033 epoch: 13\n",
      "Validation Mean Loss at 0.3785514887501077 epoch: 13\n",
      "Train Mean Loss at 0.35826635972037674 epoch: 14\n",
      "Validation Mean Loss at 0.33697967746370805 epoch: 14\n",
      "Train Mean Loss at 0.3480422688401104 epoch: 15\n",
      "Validation Mean Loss at 0.3719925775369874 epoch: 15\n",
      "Train Mean Loss at 0.3351819657385922 epoch: 16\n",
      "Validation Mean Loss at 0.34337684376586486 epoch: 16\n",
      "Train Mean Loss at 0.328745294084998 epoch: 17\n",
      "Validation Mean Loss at 0.33815334620840803 epoch: 17\n",
      "Train Mean Loss at 0.3173248255810319 epoch: 18\n",
      "Validation Mean Loss at 0.3969867309399097 epoch: 18\n",
      "Train Mean Loss at 0.31139302451323425 epoch: 19\n",
      "Validation Mean Loss at 0.3784056766188962 epoch: 19\n",
      "Train Mean Loss at 0.30310135008928335 epoch: 20\n",
      "Validation Mean Loss at 0.2800608945935296 epoch: 20\n",
      "Train Mean Loss at 0.29774048898290323 epoch: 21\n",
      "Validation Mean Loss at 0.29125781276529983 epoch: 21\n",
      "Train Mean Loss at 0.29492712716388825 epoch: 22\n",
      "Validation Mean Loss at 0.26446660166554997 epoch: 22\n",
      "Train Mean Loss at 0.29136546137507036 epoch: 23\n",
      "Validation Mean Loss at 0.26604023970983337 epoch: 23\n",
      "Train Mean Loss at 0.28170505151353076 epoch: 24\n",
      "Validation Mean Loss at 0.26677255234147096 epoch: 24\n",
      "Train Mean Loss at 0.2800817046350275 epoch: 25\n",
      "Validation Mean Loss at 0.24643041091916806 epoch: 25\n",
      "Train Mean Loss at 0.2758330837474428 epoch: 26\n",
      "Validation Mean Loss at 0.2835823605718863 epoch: 26\n",
      "Train Mean Loss at 0.27150747875690767 epoch: 27\n",
      "Validation Mean Loss at 0.25070980319718683 epoch: 27\n",
      "Train Mean Loss at 0.2685372358299004 epoch: 28\n",
      "Validation Mean Loss at 0.2591624685581642 epoch: 28\n",
      "Train Mean Loss at 0.2640303952964279 epoch: 29\n",
      "Validation Mean Loss at 0.2976689351991077 epoch: 29\n",
      "Train Mean Loss at 0.26261394325736853 epoch: 30\n",
      "Validation Mean Loss at 0.23886602529724923 epoch: 30\n",
      "Train Mean Loss at 0.2571562220458798 epoch: 31\n",
      "Validation Mean Loss at 0.31751112882751903 epoch: 31\n",
      "Train Mean Loss at 0.25593770178157804 epoch: 32\n",
      "Validation Mean Loss at 0.2661383815887721 epoch: 32\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-ae1426246b8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "print('observation size:', dataset['observations'].shape)\n",
    "print('action size:', dataset['actions'].shape)\n",
    "print('next observation size:', dataset['next_observations'].shape)\n",
    "print('rewards size:', dataset['rewards'].shape)\n",
    "print('terminal size:', dataset['terminals'].shape)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "observation size: (1998000, 17)\n",
      "action size: (1998000, 6)\n",
      "next observation size: (1998000, 17)\n",
      "rewards size: (1998000,)\n",
      "terminal size: (1998000,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('mujoco': conda)"
  },
  "interpreter": {
   "hash": "9ee2970a14e6a8f72b339ef15f848f36d69c3eb504f2e44ffee4c94d188b3a88"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}